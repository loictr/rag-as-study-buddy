{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Zephryns knowledge database\n",
    "\n",
    "Imagine an evil genius whose goal is to explore the galaxy and save endangered alien species... Not so evil after all, apart he likes seeing his failing employees suffer and uselessly beg for pity. You've just been employed at the zegma-IV station that references the Zephryn species. You now have to know this species. Otherwise, your boss will not be eager to give you your daily oxygen. \n",
    "\n",
    "All the company confidential knowledge is stored as markdown files. We have built an AI to help you. A R.A.G. is used to handle the ever growing knowledge about the studied species and to keep the knowledge confidential.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENTS_DIRECTORY=\"documents\"\n",
    "DB_DIRECTORY=\"db/chroma\"\n",
    "QUESTIONS_PATH=\"questions/questions.json\"\n",
    "EMBEDDING_MODEL='nomic-embed-text'\n",
    "LLM_MODEL=\"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "# from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# # Load markdown files from the .documents directory\n",
    "# loader = DirectoryLoader(\n",
    "#     './documents',\n",
    "#     glob=\"**/*.md\",\n",
    "#     loader_cls=UnstructuredMarkdownLoader,\n",
    "#     loader_kwargs={\"mode\": \"elements\"}\n",
    "#     )\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = DirectoryLoader(\n",
    "    DOCUMENTS_DIRECTORY,\n",
    "    glob=\"**/*.md\",\n",
    "    loader_cls=TextLoader\n",
    "    )\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from pprint import pprint\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header\"),\n",
    "    (\"##\", \"Header 1\"),\n",
    "    (\"###\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "# Markdown splitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on = headers_to_split_on,\n",
    "    strip_headers = True\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for doc in docs:\n",
    "    chunks.extend(markdown_splitter.split_text(doc.page_content))\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=['\\n\\n^(\\s*-\\s*)', '\\n^(\\s*-\\s*)', '\\n\\n', '\\n', '(?<=\\. )', ' ', ''],\n",
    ")\n",
    "\n",
    "splitted_chunks = splitter.split_documents(chunks)\n",
    "\n",
    "\n",
    "filtered_chunks = filter_complex_metadata(splitted_chunks)\n",
    "\n",
    "for item in filtered_chunks:\n",
    "    content = \"\"\n",
    "    header = item.metadata.get('Header', '')\n",
    "    if(header != \"\"):\n",
    "        content += 'Section: ' + header + '\\n'\n",
    "\n",
    "        header1 = item.metadata.get('Header 1', '')\n",
    "        if(header1 != \"\"):\n",
    "            content += 'Sub-section: ' + header1 + '\\n'\n",
    "    content += item.page_content\n",
    "\n",
    "    item.page_content = content\n",
    "    \n",
    "pprint(filtered_chunks[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Default to text-embedding-ada-002\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "\n",
    "# If the directory exists, first delete it\n",
    "try:\n",
    "    shutil.rmtree(DB_DIRECTORY)\n",
    "except FileNotFoundError as e:\n",
    "    pass\n",
    "except PermissionError:\n",
    "    db = Chroma(\n",
    "        persist_directory=DB_DIRECTORY, \n",
    "        embedding_function=embeddings)\n",
    "    db.delete_collection() # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Create vector store and save the db\n",
    "db = Chroma.from_documents(\n",
    "    filtered_chunks, \n",
    "    embeddings,\n",
    "    persist_directory=DB_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.similarity_search_with_relevance_scores(\n",
    "    \"List all the subspecies of the Zephryn.\",\n",
    "    k=10, score_threshold=0.7\n",
    ")\n",
    "pprint(results)\n",
    "\n",
    "# results = db.similarity_search_by_vector_with_relevance_scores(\n",
    "#     embeddings.embed_query(\"List all the subspecies of the Zephryn.\"),\n",
    "#     k=10#, score_threshold=0.5\n",
    "# )\n",
    "# pprint(results)\n",
    "\n",
    "\n",
    "# results = db.max_marginal_relevance_search(\n",
    "#     \"List all the subspecies of Zephryns.\",\n",
    "#     k=5#, score_threshold=0.5\n",
    "# )\n",
    "# pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain for study buddy\n",
    "\n",
    "### Generate the questions from the whole knowledge database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate some knowledge elements, to generate the questions against. Here we simply rebuild the original documents. But we could try by similarity for larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from collections import defaultdict\n",
    "llm = OllamaLLM(model=LLM_MODEL)\n",
    "\n",
    "docs = db.get(include=['metadatas'])\n",
    "\n",
    "print('items count in db: ', len(docs['ids']))\n",
    "\n",
    "# re-build the documents\n",
    "dict = defaultdict(list)\n",
    "for i, metadata in enumerate(docs['metadatas']):\n",
    "    header = metadata.get('Header')\n",
    "    id = docs['ids'][i]\n",
    "    dict[header].append(id)\n",
    "\n",
    "for item in dict:\n",
    "    print(item)\n",
    "    print(len(dict[item]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the chain to generate the question for each knowledge document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "class ByIdsRetriever(BaseRetriever):\n",
    "    _ids = []\n",
    "\n",
    "    def set_ids(self, ids):\n",
    "        self._ids = ids\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "\n",
    "        if(self._ids == []):\n",
    "            raise(\"No ids set\")\n",
    "\n",
    "        matching_documents = []\n",
    "        for id in self._ids:\n",
    "            item = db.get(id)\n",
    "\n",
    "            if(item is None or len(item.get('documents', [])) == 0):\n",
    "                continue\n",
    "\n",
    "            document = Document(\n",
    "                page_content=item['documents'][0],\n",
    "                metadata=item['metadatas'][0],\n",
    "                id=id\n",
    "            )\n",
    "\n",
    "            matching_documents.append(document)\n",
    "\n",
    "        return matching_documents\n",
    "    \n",
    "\n",
    "retriever_by_ids = ByIdsRetriever()\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_questions = PromptTemplate.from_template(\n",
    "    \"\"\"You are a study buddy. You ask the user questions that will help him to learn the concepts within the given knowledge context. \n",
    "Use the following pieces of context to generate 10 questions to ask to the user. Do not use any other information than the context.\n",
    "\n",
    "Be very concise and to the point. \n",
    "Each question expects a simple answer. \n",
    "Do not combine multiple questions in one. \n",
    "Do not start your answer with things like \"based on the context\" or \"I think\". \n",
    "Do not generate any answer to the questions.\n",
    "\n",
    "Avoid question with a yes/no answer. The question should not include any clue to their answer. \n",
    "For example \"Do Dreamweavers live in communal nests?\" or \"Can Dreamweavers be found in places of great spiritual significance?\" are bad questions because it gives the answer in the question itself. Instead, you should ask \"Where do Dreamweavers live?\" and \"What subspecies of Zephryn can be found in places of great spiritual significance?\".\n",
    "\n",
    "Consider each question as a separate question.\n",
    "\n",
    "Each question is expected to be one separate line.\n",
    "\n",
    "Context:\n",
    "---------\n",
    "{context}\n",
    "---------\n",
    "Questions: \"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        doc.page_content for doc in docs\n",
    "    )\n",
    "\n",
    "chain_questions = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"docs\"])))\n",
    "    | prompt_questions\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# get the source documents\n",
    "chain_questions_with_source = RunnableParallel(\n",
    "    {\"docs\": retriever_by_ids, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=chain_questions)\n",
    "\n",
    "\n",
    "questions_all = []\n",
    "for key, ids in dict.items():\n",
    "    retriever_by_ids.set_ids(ids)\n",
    "    llm_output = chain_questions_with_source.invoke('')\n",
    "    questions_str = llm_output['answer']\n",
    "    # split the output into individual questions by trimming and ignoring empty strings\n",
    "    questions = [q.strip() for q in questions_str.split('\\n') if q.strip()]\n",
    "    questions_all.extend(questions)\n",
    "\n",
    "\n",
    "# db_questions = Chroma.from_documents(\n",
    "#     [Document(page_content=question) for question in questions_all],\n",
    "#     embeddings,\n",
    "#     collection_name=\"questions\",\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# Store questions into a JSON file, because... why not ?\n",
    "with open(QUESTIONS_PATH, 'w') as f:\n",
    "    json.dump(questions_all, f, indent=4)\n",
    "\n",
    "\n",
    "# TODO: asks the system its own questions, for fun\n",
    "# TODO: asks the user the questions then evaluate his answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
