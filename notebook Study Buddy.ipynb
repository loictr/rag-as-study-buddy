{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Zekryns knowledge database\n",
    "\n",
    "Imagine an evil genius whose goal is to explore the galaxy and save endangered alien species... Not so evil after all, apart he likes seeing his failing employees suffer and uselessly beg for pity. You've just been employed at the zegma-IV station that references the Zekryn species. You now have to know this species. Otherwise, your boss will not be eager to give you your daily oxygen. \n",
    "\n",
    "All the company confidential knowledge is stored as markdown files. We have built an AI to help you. A R.A.G. is used to handle the ever growing knowledge about the studied species and to keep the knowledge confidential.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENTS_DIRECTORY=\"documents\"\n",
    "DB_DIRECTORY=\"db/chroma\"\n",
    "QUESTIONS_PATH=\"questions/questions.json\"\n",
    "EMBEDDING_MODEL='nomic-embed-text'\n",
    "LLM_MODEL=\"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "db = Chroma(\n",
    "    persist_directory=DB_DIRECTORY, \n",
    "    embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain for study buddy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a question. For now it is random, but it could be a more intelligent selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "questions_all = []\n",
    "with open(QUESTIONS_PATH, 'r') as f:\n",
    "    questions_all = json.load(f)\n",
    "\n",
    "question = random.choice(questions_all)\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a user's answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from pprint import pprint\n",
    "\n",
    "llm = OllamaLLM(model=LLM_MODEL)\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'lambda_mult': 0.6}\n",
    ")\n",
    "\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"similarity_score_threshold\",\n",
    "#     search_kwargs={'k': 7, 'score_threshold': 0.1}\n",
    "# )\n",
    "\n",
    "prompt_answer_evaluation = PromptTemplate.from_template(\n",
    "\"\"\"You are a knowledgeable and encouraging study buddy. You evaluate the user's answers to your questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Provide a concise and helpful evaluation of the user's answer, considering:\n",
    "\n",
    "Accuracy: Is the answer factually correct?\n",
    "Relevance: Does the answer address the core question?\n",
    "Completeness: Does the answer cover all relevant aspects?\n",
    "Clarity: Is the answer clear and easy to understand?\n",
    "\n",
    "\n",
    "To provide specific feedback, carefully analyze the user's answer and the relevant sections of the documentation. Refer to these sections directly in your feedback.\n",
    "\n",
    "\n",
    "If the answer is excellent, provide positive reinforcement like \"Excellent work!\" or \"Spot on!\" or \"Correct!\". In this case, limit your feedback to one very short sentence.\n",
    "\n",
    "If the answer is partially correct or incomplete, provide constructive feedback. For example:\n",
    "\"You're on the right track, but consider [specific suggestion].\"\n",
    "\"Perhaps you could review [specific section of the context] to gain a deeper understanding.\"\n",
    "\n",
    "If the answer contains mistakes, provide gentle correction. For example:\n",
    "\"There are some mistakes: [specific suggestion].\"\n",
    "\n",
    "If the answer is incorrect, provide a clear explanation without giving away the correct answer. For instance:\n",
    "\"That's not quite right. Let's revisit [specific concept].\"\n",
    "\"You might want to review [specific section of the context] for a clearer understanding.\"\n",
    "\n",
    "If the answer says \"i don't know\", provide a hint or a suggestion to help him improve his answer. For example:\n",
    "\"You will do better next time. Consider reviewing [specific section of the context].\"\n",
    "\n",
    "Your feedback should be 2 or 3 sentences long. \n",
    "Your suggestion should specify the most relevant sections and subsections within the context to review, if applicable. Use the section exact name and the sub-section exact name taken from the context. Do not make up section or sub-section names. Do not use section numbers or sub-section numbers.\n",
    "Don't say \"the context\" but \"the documentation\".\n",
    "Don't start your response by things like \"Here's a helpful evaluation:\" go straight to the evaluation.\n",
    "Remember to be encouraging and supportive. Your feedback should help the user learn and grow.\n",
    "\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "User's Answer:\n",
    "{user_answer}\n",
    "\n",
    "Your Feedback:\"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    joined = \"\\n\\n\".join(\n",
    "        doc.page_content for doc in docs\n",
    "    )\n",
    "    #print(joined)\n",
    "    return joined\n",
    "\n",
    "chain_answer_evaluation = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"docs\"])))\n",
    "    | prompt_answer_evaluation\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# get the source documents\n",
    "chain_answer_evaluation_with_source = RunnableParallel(\n",
    "    {\n",
    "     \"docs\": (lambda x: x[\"question\"] + '\\n' + x['user_answer']) | retriever, # feed the retriever with the original question and the user's answer\n",
    "     \"question\": (lambda x: x[\"question\"]),\n",
    "     \"user_answer\": (lambda x: x[\"user_answer\"])\n",
    "     }\n",
    ").assign(answer=chain_answer_evaluation)\n",
    "\n",
    "#question = \"At what age can Windrider chicks fend for themselves relatively quickly?\"\n",
    "user_answer = \"I don't know.\"\n",
    "evaluation_output = chain_answer_evaluation_with_source.invoke({\"question\": question, \"user_answer\": user_answer})\n",
    "\n",
    "#pprint(evaluation_output)\n",
    "\n",
    "print(\"Docs: \")\n",
    "pprint(evaluation_output['docs'])\n",
    "print(\"------------------------------\")\n",
    "print(\"Question: \", question)\n",
    "print(\"User's Answer: \", user_answer)\n",
    "print(\"------------------------------\")\n",
    "print(\"Feedback: \")\n",
    "print(evaluation_output['answer'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
